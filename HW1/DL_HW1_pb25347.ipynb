{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b531c2dc",
      "metadata": {
        "id": "b531c2dc"
      },
      "source": [
        "# HW1\n",
        "\n",
        "Submitted by:\n",
        "\n",
        "Parthiv Borgohain (pb25347)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c274da66",
      "metadata": {
        "id": "c274da66"
      },
      "source": [
        "## Q1.1 \n",
        "For the function y = f(x) = w.x + b, where w=[2,1] and b=3, \u0000nd the partial derivates of y w.r.t the components of x (i.e.: dy/dx1 and dy/dx2) at x = [4,2] both on paper and using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "658203dc",
      "metadata": {
        "id": "658203dc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "x=torch.tensor([4,2], dtype=torch.float, requires_grad=True) \n",
        "w=torch.tensor([2,1],dtype=torch.float)\n",
        "b=torch.tensor(3,dtype=torch.float)\n",
        "y=torch.matmul(w.T,x) + b\n",
        "y.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b68866d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b68866d9",
        "outputId": "1d0fb7fd-47e8-49c1-c562-82a8eb05c2b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#Now let us compute the gradient\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, the two gradients w.r.t x1 and x2 are **2 and 1** respectively"
      ],
      "metadata": {
        "id": "6rO6pH-LD3Fv"
      },
      "id": "6rO6pH-LD3Fv"
    },
    {
      "cell_type": "markdown",
      "id": "55cbea6d",
      "metadata": {
        "id": "55cbea6d"
      },
      "source": [
        "## 1.2\n",
        "\n",
        "For a model y = f(x), the predicted and true values are as follows: \n",
        "\n",
        "y_true = [0,1,1,0] \n",
        "\n",
        "y_pred = [0.1,0.95,1.10,0.2] \n",
        "\n",
        "Find the mean squared error both on paper and using PyTorch. In PyTorch, solve it using an inbuilt function and also by defining your own squared error function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a5a6fabe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5a6fabe",
        "outputId": "91f27b24-5c6f-4c2b-803d-9b34881a1943"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0156)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#Let us try to find Mean Square Error using an inbuilt function in Pytorch\n",
        "y_true=torch.tensor([0,1,1,0],dtype=torch.float)\n",
        "y_pred=torch.tensor([0.1,0.95,1.10,0.2])\n",
        "loss=torch.nn.MSELoss()\n",
        "loss(y_true,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us define a custom MSE function\n",
        "def mseLoss(y_true,y_pred):\n",
        "    mseloss=torch.mean((y_true-y_pred)*(y_true-y_pred))\n",
        "    return mseloss"
      ],
      "metadata": {
        "id": "Z_mvH7YMBbOr"
      },
      "id": "Z_mvH7YMBbOr",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss=mseLoss(y_true,y_pred)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYcQPqjEBc6m",
        "outputId": "2eef86c1-d420-4e6f-fa47-845b8a57a497"
      },
      "id": "TYcQPqjEBc6m",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0156)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the MSE loss is obtained by both methods is approximately **0.0156**"
      ],
      "metadata": {
        "id": "tW5WdLPGEVWx"
      },
      "id": "tW5WdLPGEVWx"
    },
    {
      "cell_type": "markdown",
      "id": "7d535b27",
      "metadata": {
        "id": "7d535b27"
      },
      "source": [
        "## 1.3\n",
        "\n",
        "You are given the following dataset of an XOR function: Assume that we fit a linear function, y = w.x + b to this dataset with w = [2,1] and b=3\n",
        "\n",
        "i) Find the mean squared error (loss) over this dataset for the above weights and bias.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "707b3216",
      "metadata": {
        "id": "707b3216"
      },
      "outputs": [],
      "source": [
        "x=torch.tensor([[0,0],[0,1],[1,0],[1,1]],dtype=torch.float)\n",
        "y_true=torch.tensor([0,1,1,0],dtype=torch.float)\n",
        "w=torch.tensor([2,1],dtype=torch.float,requires_grad=True)\n",
        "b=torch.tensor(3.0,requires_grad=True)\n",
        "y_pred=torch.sum(w*x,1)+b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "113fb210",
      "metadata": {
        "id": "113fb210"
      },
      "outputs": [],
      "source": [
        "def mse_loss(y_true,y_pred):\n",
        "    mseloss=torch.mean((y_true-y_pred)*(y_true-y_pred))\n",
        "    return mseloss\n",
        "def model(X):\n",
        "    return torch.matmul(X,w.T) + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b1e4f702",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1e4f702",
        "outputId": "3f08d524-f131-43cf-bcde-4618cf05c97e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.5000, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "loss=mse_loss(y_true,y_pred)\n",
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the MSE obtained is **17.5** approximately"
      ],
      "metadata": {
        "id": "luWEzo0NE49a"
      },
      "id": "luWEzo0NE49a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Which direction should the weights move in to decrease the loss by maximum amount? Find that direction both on paper and using PyTorch.\n",
        "\n"
      ],
      "metadata": {
        "id": "8dJA63uK2TWU"
      },
      "id": "8dJA63uK2TWU"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "91105a4a",
      "metadata": {
        "id": "91105a4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85b822a4-601e-4217-de1f-2031ef210ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([5.0000, 4.5000])\n",
            "tensor(8.)\n"
          ]
        }
      ],
      "source": [
        "data = TensorDataset(x, y_true)\n",
        "print(loss.backward())\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd5cbfab",
      "metadata": {
        "id": "fd5cbfab"
      },
      "source": [
        "Gradients w.r.t w1 and w2 are **5 and 4.5** \n",
        "\n",
        "Gradient w.r.t b is **8**\n",
        "\n",
        "The direction vector is **[5,4.5,8]**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) For what values of w and b is the loss minimum? Solve on paper only.\n",
        "\n",
        "Has been solved in the attached pdf file"
      ],
      "metadata": {
        "id": "FFjjCRugJK2E"
      },
      "id": "FFjjCRugJK2E"
    },
    {
      "cell_type": "markdown",
      "id": "dfbcc943",
      "metadata": {
        "id": "dfbcc943"
      },
      "source": [
        "### Extra credit\n",
        "Can you solve for the optimum w and b using PyTorch? Hint: use gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "add027b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "add027b9",
        "outputId": "89fe9b8b-33b9-42c3-8f34-05de8b5b5099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000: Loss: 17.5\n",
            "Epoch 1/1000: Loss: 2.6050000190734863\n",
            "Epoch 2/1000: Loss: 1.3877873420715332\n",
            "Epoch 3/1000: Loss: 0.8183243274688721\n",
            "Epoch 4/1000: Loss: 0.5500246286392212\n",
            "Epoch 5/1000: Loss: 0.42193183302879333\n",
            "Epoch 6/1000: Loss: 0.3592788577079773\n",
            "Epoch 7/1000: Loss: 0.32731887698173523\n",
            "Epoch 8/1000: Loss: 0.30988913774490356\n",
            "Epoch 9/1000: Loss: 0.29945817589759827\n",
            "Epoch 10/1000: Loss: 0.29250460863113403\n",
            "Epoch 11/1000: Loss: 0.28737133741378784\n",
            "Epoch 12/1000: Loss: 0.28326886892318726\n",
            "Epoch 13/1000: Loss: 0.2798125743865967\n",
            "Epoch 14/1000: Loss: 0.27680760622024536\n",
            "Epoch 15/1000: Loss: 0.2741485834121704\n",
            "Epoch 16/1000: Loss: 0.27177348732948303\n",
            "Epoch 17/1000: Loss: 0.26964133977890015\n",
            "Epoch 18/1000: Loss: 0.2677222788333893\n",
            "Epoch 19/1000: Loss: 0.26599282026290894\n",
            "Epoch 20/1000: Loss: 0.2644330561161041\n",
            "Epoch 21/1000: Loss: 0.2630259096622467\n",
            "Epoch 22/1000: Loss: 0.26175615191459656\n",
            "Epoch 23/1000: Loss: 0.26061028242111206\n",
            "Epoch 24/1000: Loss: 0.2595761716365814\n",
            "Epoch 25/1000: Loss: 0.25864288210868835\n",
            "Epoch 26/1000: Loss: 0.25780054926872253\n",
            "Epoch 27/1000: Loss: 0.2570403516292572\n",
            "Epoch 28/1000: Loss: 0.2563542425632477\n",
            "Epoch 29/1000: Loss: 0.25573500990867615\n",
            "Epoch 30/1000: Loss: 0.2551761269569397\n",
            "Epoch 31/1000: Loss: 0.25467172265052795\n",
            "Epoch 32/1000: Loss: 0.2542164921760559\n",
            "Epoch 33/1000: Loss: 0.25380563735961914\n",
            "Epoch 34/1000: Loss: 0.25343477725982666\n",
            "Epoch 35/1000: Loss: 0.25310009717941284\n",
            "Epoch 36/1000: Loss: 0.25279808044433594\n",
            "Epoch 37/1000: Loss: 0.2525254487991333\n",
            "Epoch 38/1000: Loss: 0.2522793710231781\n",
            "Epoch 39/1000: Loss: 0.2520573139190674\n",
            "Epoch 40/1000: Loss: 0.25185686349868774\n",
            "Epoch 41/1000: Loss: 0.25167596340179443\n",
            "Epoch 42/1000: Loss: 0.25151270627975464\n",
            "Epoch 43/1000: Loss: 0.2513653337955475\n",
            "Epoch 44/1000: Loss: 0.2512323558330536\n",
            "Epoch 45/1000: Loss: 0.25111228227615356\n",
            "Epoch 46/1000: Loss: 0.2510039210319519\n",
            "Epoch 47/1000: Loss: 0.2509061396121979\n",
            "Epoch 48/1000: Loss: 0.2508178949356079\n",
            "Epoch 49/1000: Loss: 0.250738263130188\n",
            "Epoch 50/1000: Loss: 0.2506663501262665\n",
            "Epoch 51/1000: Loss: 0.25060147047042847\n",
            "Epoch 52/1000: Loss: 0.25054290890693665\n",
            "Epoch 53/1000: Loss: 0.2504900395870209\n",
            "Epoch 54/1000: Loss: 0.2504423260688782\n",
            "Epoch 55/1000: Loss: 0.25039923191070557\n",
            "Epoch 56/1000: Loss: 0.250360369682312\n",
            "Epoch 57/1000: Loss: 0.2503252923488617\n",
            "Epoch 58/1000: Loss: 0.2502936124801636\n",
            "Epoch 59/1000: Loss: 0.2502650320529938\n",
            "Epoch 60/1000: Loss: 0.25023922324180603\n",
            "Epoch 61/1000: Loss: 0.25021594762802124\n",
            "Epoch 62/1000: Loss: 0.2501949369907379\n",
            "Epoch 63/1000: Loss: 0.25017595291137695\n",
            "Epoch 64/1000: Loss: 0.2501588463783264\n",
            "Epoch 65/1000: Loss: 0.2501434087753296\n",
            "Epoch 66/1000: Loss: 0.2501294016838074\n",
            "Epoch 67/1000: Loss: 0.25011682510375977\n",
            "Epoch 68/1000: Loss: 0.25010550022125244\n",
            "Epoch 69/1000: Loss: 0.2500952184200287\n",
            "Epoch 70/1000: Loss: 0.2500859498977661\n",
            "Epoch 71/1000: Loss: 0.25007760524749756\n",
            "Epoch 72/1000: Loss: 0.25007006525993347\n",
            "Epoch 73/1000: Loss: 0.2500632405281067\n",
            "Epoch 74/1000: Loss: 0.25005707144737244\n",
            "Epoch 75/1000: Loss: 0.2500515282154083\n",
            "Epoch 76/1000: Loss: 0.2500465214252472\n",
            "Epoch 77/1000: Loss: 0.25004202127456665\n",
            "Epoch 78/1000: Loss: 0.25003793835639954\n",
            "Epoch 79/1000: Loss: 0.25003424286842346\n",
            "Epoch 80/1000: Loss: 0.25003090500831604\n",
            "Epoch 81/1000: Loss: 0.25002792477607727\n",
            "Epoch 82/1000: Loss: 0.25002521276474\n",
            "Epoch 83/1000: Loss: 0.2500227689743042\n",
            "Epoch 84/1000: Loss: 0.2500205338001251\n",
            "Epoch 85/1000: Loss: 0.25001853704452515\n",
            "Epoch 86/1000: Loss: 0.2500167489051819\n",
            "Epoch 87/1000: Loss: 0.25001513957977295\n",
            "Epoch 88/1000: Loss: 0.25001364946365356\n",
            "Epoch 89/1000: Loss: 0.2500123083591461\n",
            "Epoch 90/1000: Loss: 0.250011146068573\n",
            "Epoch 91/1000: Loss: 0.25001004338264465\n",
            "Epoch 92/1000: Loss: 0.25000908970832825\n",
            "Epoch 93/1000: Loss: 0.250008225440979\n",
            "Epoch 94/1000: Loss: 0.25000739097595215\n",
            "Epoch 95/1000: Loss: 0.25000667572021484\n",
            "Epoch 96/1000: Loss: 0.2500060498714447\n",
            "Epoch 97/1000: Loss: 0.25000545382499695\n",
            "Epoch 98/1000: Loss: 0.25000491738319397\n",
            "Epoch 99/1000: Loss: 0.25000447034835815\n",
            "Epoch 100/1000: Loss: 0.25000402331352234\n",
            "Epoch 101/1000: Loss: 0.2500036358833313\n",
            "Epoch 102/1000: Loss: 0.25000327825546265\n",
            "Epoch 103/1000: Loss: 0.25000298023223877\n",
            "Epoch 104/1000: Loss: 0.2500026822090149\n",
            "Epoch 105/1000: Loss: 0.2500024437904358\n",
            "Epoch 106/1000: Loss: 0.2500021755695343\n",
            "Epoch 107/1000: Loss: 0.2500019669532776\n",
            "Epoch 108/1000: Loss: 0.25000178813934326\n",
            "Epoch 109/1000: Loss: 0.25000160932540894\n",
            "Epoch 110/1000: Loss: 0.250001460313797\n",
            "Epoch 111/1000: Loss: 0.25000131130218506\n",
            "Epoch 112/1000: Loss: 0.2500011622905731\n",
            "Epoch 113/1000: Loss: 0.25000107288360596\n",
            "Epoch 114/1000: Loss: 0.2500009834766388\n",
            "Epoch 115/1000: Loss: 0.25000089406967163\n",
            "Epoch 116/1000: Loss: 0.25000080466270447\n",
            "Epoch 117/1000: Loss: 0.2500007152557373\n",
            "Epoch 118/1000: Loss: 0.25000065565109253\n",
            "Epoch 119/1000: Loss: 0.25000056624412537\n",
            "Epoch 120/1000: Loss: 0.2500005066394806\n",
            "Epoch 121/1000: Loss: 0.2500005066394806\n",
            "Epoch 122/1000: Loss: 0.2500004470348358\n",
            "Epoch 123/1000: Loss: 0.25000038743019104\n",
            "Epoch 124/1000: Loss: 0.25000032782554626\n",
            "Epoch 125/1000: Loss: 0.2500002980232239\n",
            "Epoch 126/1000: Loss: 0.2500002980232239\n",
            "Epoch 127/1000: Loss: 0.2500002682209015\n",
            "Epoch 128/1000: Loss: 0.2500002384185791\n",
            "Epoch 129/1000: Loss: 0.2500002086162567\n",
            "Epoch 130/1000: Loss: 0.2500002086162567\n",
            "Epoch 131/1000: Loss: 0.2500001788139343\n",
            "Epoch 132/1000: Loss: 0.25000014901161194\n",
            "Epoch 133/1000: Loss: 0.25000011920928955\n",
            "Epoch 134/1000: Loss: 0.25000011920928955\n",
            "Epoch 135/1000: Loss: 0.25000011920928955\n",
            "Epoch 136/1000: Loss: 0.25000008940696716\n",
            "Epoch 137/1000: Loss: 0.25000011920928955\n",
            "Epoch 138/1000: Loss: 0.25000008940696716\n",
            "Epoch 139/1000: Loss: 0.2500000596046448\n",
            "Epoch 140/1000: Loss: 0.2500000596046448\n",
            "Epoch 141/1000: Loss: 0.2500000596046448\n",
            "Epoch 142/1000: Loss: 0.2500000596046448\n",
            "Epoch 143/1000: Loss: 0.2500000596046448\n",
            "Epoch 144/1000: Loss: 0.2500000298023224\n",
            "Epoch 145/1000: Loss: 0.2500000298023224\n",
            "Epoch 146/1000: Loss: 0.2500000298023224\n",
            "Epoch 147/1000: Loss: 0.2500000298023224\n",
            "Epoch 148/1000: Loss: 0.2500000298023224\n",
            "Epoch 149/1000: Loss: 0.2500000298023224\n",
            "Epoch 150/1000: Loss: 0.2500000596046448\n",
            "Epoch 151/1000: Loss: 0.2500000298023224\n",
            "Epoch 152/1000: Loss: 0.25\n",
            "Epoch 153/1000: Loss: 0.2500000298023224\n",
            "Epoch 154/1000: Loss: 0.25\n",
            "Epoch 155/1000: Loss: 0.25\n",
            "Epoch 156/1000: Loss: 0.2500000298023224\n",
            "Epoch 157/1000: Loss: 0.2500000298023224\n",
            "Epoch 158/1000: Loss: 0.25\n",
            "Epoch 159/1000: Loss: 0.25\n",
            "Epoch 160/1000: Loss: 0.25\n",
            "Epoch 161/1000: Loss: 0.2500000298023224\n",
            "Epoch 162/1000: Loss: 0.2500000298023224\n",
            "Epoch 163/1000: Loss: 0.25\n",
            "Epoch 164/1000: Loss: 0.25\n",
            "Epoch 165/1000: Loss: 0.25\n",
            "Epoch 166/1000: Loss: 0.25\n",
            "Epoch 167/1000: Loss: 0.25\n",
            "Epoch 168/1000: Loss: 0.2499999701976776\n",
            "Epoch 169/1000: Loss: 0.25\n",
            "Epoch 170/1000: Loss: 0.25\n",
            "Epoch 171/1000: Loss: 0.2499999850988388\n",
            "Epoch 172/1000: Loss: 0.25\n",
            "Epoch 173/1000: Loss: 0.25\n",
            "Epoch 174/1000: Loss: 0.25\n",
            "Epoch 175/1000: Loss: 0.25\n",
            "Epoch 176/1000: Loss: 0.2499999850988388\n",
            "Epoch 177/1000: Loss: 0.2499999850988388\n",
            "Epoch 178/1000: Loss: 0.2499999850988388\n",
            "Epoch 179/1000: Loss: 0.2499999850988388\n",
            "Epoch 180/1000: Loss: 0.25\n",
            "Epoch 181/1000: Loss: 0.2499999850988388\n",
            "Epoch 182/1000: Loss: 0.2499999850988388\n",
            "Epoch 183/1000: Loss: 0.25\n",
            "Epoch 184/1000: Loss: 0.25\n",
            "Epoch 185/1000: Loss: 0.25\n",
            "Epoch 186/1000: Loss: 0.2499999850988388\n",
            "Epoch 187/1000: Loss: 0.25\n",
            "Epoch 188/1000: Loss: 0.25\n",
            "Epoch 189/1000: Loss: 0.25\n",
            "Epoch 190/1000: Loss: 0.2499999850988388\n",
            "Epoch 191/1000: Loss: 0.25\n",
            "Epoch 192/1000: Loss: 0.25\n",
            "Epoch 193/1000: Loss: 0.25\n",
            "Epoch 194/1000: Loss: 0.25\n",
            "Epoch 195/1000: Loss: 0.25\n",
            "Epoch 196/1000: Loss: 0.25\n",
            "Epoch 197/1000: Loss: 0.25\n",
            "Epoch 198/1000: Loss: 0.25\n",
            "Epoch 199/1000: Loss: 0.25\n",
            "Epoch 200/1000: Loss: 0.25\n",
            "Epoch 201/1000: Loss: 0.25\n",
            "Epoch 202/1000: Loss: 0.25\n",
            "Epoch 203/1000: Loss: 0.25\n",
            "Epoch 204/1000: Loss: 0.25\n",
            "Epoch 205/1000: Loss: 0.25\n",
            "Epoch 206/1000: Loss: 0.25\n",
            "Epoch 207/1000: Loss: 0.2499999850988388\n",
            "Epoch 208/1000: Loss: 0.25\n",
            "Epoch 209/1000: Loss: 0.25\n",
            "Epoch 210/1000: Loss: 0.25\n",
            "Epoch 211/1000: Loss: 0.25\n",
            "Epoch 212/1000: Loss: 0.25\n",
            "Epoch 213/1000: Loss: 0.25\n",
            "Epoch 214/1000: Loss: 0.25\n",
            "Epoch 215/1000: Loss: 0.25\n",
            "Epoch 216/1000: Loss: 0.25\n",
            "Epoch 217/1000: Loss: 0.25\n",
            "Epoch 218/1000: Loss: 0.25\n",
            "Epoch 219/1000: Loss: 0.25\n",
            "Epoch 220/1000: Loss: 0.25\n",
            "Epoch 221/1000: Loss: 0.25\n",
            "Epoch 222/1000: Loss: 0.25\n",
            "Epoch 223/1000: Loss: 0.25\n",
            "Epoch 224/1000: Loss: 0.25\n",
            "Epoch 225/1000: Loss: 0.25\n",
            "Epoch 226/1000: Loss: 0.2499999850988388\n",
            "Epoch 227/1000: Loss: 0.25\n",
            "Epoch 228/1000: Loss: 0.25\n",
            "Epoch 229/1000: Loss: 0.2500000298023224\n",
            "Epoch 230/1000: Loss: 0.25\n",
            "Epoch 231/1000: Loss: 0.25\n",
            "Epoch 232/1000: Loss: 0.25\n",
            "Epoch 233/1000: Loss: 0.25\n",
            "Epoch 234/1000: Loss: 0.2499999850988388\n",
            "Epoch 235/1000: Loss: 0.25\n",
            "Epoch 236/1000: Loss: 0.25\n",
            "Epoch 237/1000: Loss: 0.25\n",
            "Epoch 238/1000: Loss: 0.25\n",
            "Epoch 239/1000: Loss: 0.25\n",
            "Epoch 240/1000: Loss: 0.25\n",
            "Epoch 241/1000: Loss: 0.25\n",
            "Epoch 242/1000: Loss: 0.25\n",
            "Epoch 243/1000: Loss: 0.25\n",
            "Epoch 244/1000: Loss: 0.25\n",
            "Epoch 245/1000: Loss: 0.25\n",
            "Epoch 246/1000: Loss: 0.25\n",
            "Epoch 247/1000: Loss: 0.2499999850988388\n",
            "Epoch 248/1000: Loss: 0.25\n",
            "Epoch 249/1000: Loss: 0.25\n",
            "Epoch 250/1000: Loss: 0.2499999850988388\n",
            "Epoch 251/1000: Loss: 0.25\n",
            "Epoch 252/1000: Loss: 0.25\n",
            "Epoch 253/1000: Loss: 0.25\n",
            "Epoch 254/1000: Loss: 0.25\n",
            "Epoch 255/1000: Loss: 0.25\n",
            "Epoch 256/1000: Loss: 0.25\n",
            "Epoch 257/1000: Loss: 0.25\n",
            "Epoch 258/1000: Loss: 0.25\n",
            "Epoch 259/1000: Loss: 0.25\n",
            "Epoch 260/1000: Loss: 0.25\n",
            "Epoch 261/1000: Loss: 0.25\n",
            "Epoch 262/1000: Loss: 0.25\n",
            "Epoch 263/1000: Loss: 0.25\n",
            "Epoch 264/1000: Loss: 0.25\n",
            "Epoch 265/1000: Loss: 0.25\n",
            "Epoch 266/1000: Loss: 0.25\n",
            "Epoch 267/1000: Loss: 0.25\n",
            "Epoch 268/1000: Loss: 0.25\n",
            "Epoch 269/1000: Loss: 0.25\n",
            "Epoch 270/1000: Loss: 0.25\n",
            "Epoch 271/1000: Loss: 0.25\n",
            "Epoch 272/1000: Loss: 0.2499999850988388\n",
            "Epoch 273/1000: Loss: 0.25\n",
            "Epoch 274/1000: Loss: 0.25\n",
            "Epoch 275/1000: Loss: 0.25\n",
            "Epoch 276/1000: Loss: 0.2499999850988388\n",
            "Epoch 277/1000: Loss: 0.2499999850988388\n",
            "Epoch 278/1000: Loss: 0.25\n",
            "Epoch 279/1000: Loss: 0.25\n",
            "Epoch 280/1000: Loss: 0.2499999850988388\n",
            "Epoch 281/1000: Loss: 0.25\n",
            "Epoch 282/1000: Loss: 0.25\n",
            "Epoch 283/1000: Loss: 0.25\n",
            "Epoch 284/1000: Loss: 0.25\n",
            "Epoch 285/1000: Loss: 0.25\n",
            "Epoch 286/1000: Loss: 0.25\n",
            "Epoch 287/1000: Loss: 0.2499999850988388\n",
            "Epoch 288/1000: Loss: 0.2499999701976776\n",
            "Epoch 289/1000: Loss: 0.2499999850988388\n",
            "Epoch 290/1000: Loss: 0.25\n",
            "Epoch 291/1000: Loss: 0.25\n",
            "Epoch 292/1000: Loss: 0.25\n",
            "Epoch 293/1000: Loss: 0.25\n",
            "Epoch 294/1000: Loss: 0.2499999850988388\n",
            "Epoch 295/1000: Loss: 0.2499999850988388\n",
            "Epoch 296/1000: Loss: 0.2499999850988388\n",
            "Epoch 297/1000: Loss: 0.25\n",
            "Epoch 298/1000: Loss: 0.2499999850988388\n",
            "Epoch 299/1000: Loss: 0.25\n",
            "Epoch 300/1000: Loss: 0.2499999850988388\n",
            "Epoch 301/1000: Loss: 0.25\n",
            "Epoch 302/1000: Loss: 0.25\n",
            "Epoch 303/1000: Loss: 0.2499999850988388\n",
            "Epoch 304/1000: Loss: 0.25\n",
            "Epoch 305/1000: Loss: 0.2499999701976776\n",
            "Epoch 306/1000: Loss: 0.2499999850988388\n",
            "Epoch 307/1000: Loss: 0.25\n",
            "Epoch 308/1000: Loss: 0.2499999850988388\n",
            "Epoch 309/1000: Loss: 0.2499999850988388\n",
            "Epoch 310/1000: Loss: 0.25\n",
            "Epoch 311/1000: Loss: 0.25\n",
            "Epoch 312/1000: Loss: 0.2499999850988388\n",
            "Epoch 313/1000: Loss: 0.2499999850988388\n",
            "Epoch 314/1000: Loss: 0.25\n",
            "Epoch 315/1000: Loss: 0.25\n",
            "Epoch 316/1000: Loss: 0.25\n",
            "Epoch 317/1000: Loss: 0.2499999850988388\n",
            "Epoch 318/1000: Loss: 0.25\n",
            "Epoch 319/1000: Loss: 0.25\n",
            "Epoch 320/1000: Loss: 0.25\n",
            "Epoch 321/1000: Loss: 0.25\n",
            "Epoch 322/1000: Loss: 0.25\n",
            "Epoch 323/1000: Loss: 0.25\n",
            "Epoch 324/1000: Loss: 0.25\n",
            "Epoch 325/1000: Loss: 0.25\n",
            "Epoch 326/1000: Loss: 0.25\n",
            "Epoch 327/1000: Loss: 0.25\n",
            "Epoch 328/1000: Loss: 0.25\n",
            "Epoch 329/1000: Loss: 0.25\n",
            "Epoch 330/1000: Loss: 0.25\n",
            "Epoch 331/1000: Loss: 0.25\n",
            "Epoch 332/1000: Loss: 0.25\n",
            "Epoch 333/1000: Loss: 0.2499999850988388\n",
            "Epoch 334/1000: Loss: 0.25\n",
            "Epoch 335/1000: Loss: 0.25\n",
            "Epoch 336/1000: Loss: 0.25\n",
            "Epoch 337/1000: Loss: 0.25\n",
            "Epoch 338/1000: Loss: 0.25\n",
            "Epoch 339/1000: Loss: 0.25\n",
            "Epoch 340/1000: Loss: 0.25\n",
            "Epoch 341/1000: Loss: 0.2499999850988388\n",
            "Epoch 342/1000: Loss: 0.25\n",
            "Epoch 343/1000: Loss: 0.2499999850988388\n",
            "Epoch 344/1000: Loss: 0.2499999850988388\n",
            "Epoch 345/1000: Loss: 0.25\n",
            "Epoch 346/1000: Loss: 0.25\n",
            "Epoch 347/1000: Loss: 0.25\n",
            "Epoch 348/1000: Loss: 0.25\n",
            "Epoch 349/1000: Loss: 0.25\n",
            "Epoch 350/1000: Loss: 0.25\n",
            "Epoch 351/1000: Loss: 0.25\n",
            "Epoch 352/1000: Loss: 0.25\n",
            "Epoch 353/1000: Loss: 0.25\n",
            "Epoch 354/1000: Loss: 0.25\n",
            "Epoch 355/1000: Loss: 0.25\n",
            "Epoch 356/1000: Loss: 0.2499999701976776\n",
            "Epoch 357/1000: Loss: 0.25\n",
            "Epoch 358/1000: Loss: 0.25\n",
            "Epoch 359/1000: Loss: 0.25\n",
            "Epoch 360/1000: Loss: 0.25\n",
            "Epoch 361/1000: Loss: 0.25\n",
            "Epoch 362/1000: Loss: 0.2499999850988388\n",
            "Epoch 363/1000: Loss: 0.2499999850988388\n",
            "Epoch 364/1000: Loss: 0.2499999850988388\n",
            "Epoch 365/1000: Loss: 0.2499999850988388\n",
            "Epoch 366/1000: Loss: 0.25\n",
            "Epoch 367/1000: Loss: 0.25\n",
            "Epoch 368/1000: Loss: 0.25\n",
            "Epoch 369/1000: Loss: 0.25\n",
            "Epoch 370/1000: Loss: 0.25\n",
            "Epoch 371/1000: Loss: 0.25\n",
            "Epoch 372/1000: Loss: 0.25\n",
            "Epoch 373/1000: Loss: 0.25\n",
            "Epoch 374/1000: Loss: 0.25\n",
            "Epoch 375/1000: Loss: 0.25\n",
            "Epoch 376/1000: Loss: 0.25\n",
            "Epoch 377/1000: Loss: 0.25\n",
            "Epoch 378/1000: Loss: 0.25\n",
            "Epoch 379/1000: Loss: 0.25\n",
            "Epoch 380/1000: Loss: 0.25\n",
            "Epoch 381/1000: Loss: 0.25\n",
            "Epoch 382/1000: Loss: 0.25\n",
            "Epoch 383/1000: Loss: 0.25\n",
            "Epoch 384/1000: Loss: 0.25\n",
            "Epoch 385/1000: Loss: 0.25\n",
            "Epoch 386/1000: Loss: 0.25\n",
            "Epoch 387/1000: Loss: 0.25\n",
            "Epoch 388/1000: Loss: 0.25\n",
            "Epoch 389/1000: Loss: 0.25\n",
            "Epoch 390/1000: Loss: 0.25\n",
            "Epoch 391/1000: Loss: 0.25\n",
            "Epoch 392/1000: Loss: 0.25\n",
            "Epoch 393/1000: Loss: 0.25\n",
            "Epoch 394/1000: Loss: 0.25\n",
            "Epoch 395/1000: Loss: 0.25\n",
            "Epoch 396/1000: Loss: 0.25\n",
            "Epoch 397/1000: Loss: 0.25\n",
            "Epoch 398/1000: Loss: 0.25\n",
            "Epoch 399/1000: Loss: 0.25\n",
            "Epoch 400/1000: Loss: 0.25\n",
            "Epoch 401/1000: Loss: 0.25\n",
            "Epoch 402/1000: Loss: 0.25\n",
            "Epoch 403/1000: Loss: 0.25\n",
            "Epoch 404/1000: Loss: 0.25\n",
            "Epoch 405/1000: Loss: 0.25\n",
            "Epoch 406/1000: Loss: 0.25\n",
            "Epoch 407/1000: Loss: 0.25\n",
            "Epoch 408/1000: Loss: 0.25\n",
            "Epoch 409/1000: Loss: 0.25\n",
            "Epoch 410/1000: Loss: 0.25\n",
            "Epoch 411/1000: Loss: 0.25\n",
            "Epoch 412/1000: Loss: 0.25\n",
            "Epoch 413/1000: Loss: 0.25\n",
            "Epoch 414/1000: Loss: 0.25\n",
            "Epoch 415/1000: Loss: 0.25\n",
            "Epoch 416/1000: Loss: 0.25\n",
            "Epoch 417/1000: Loss: 0.25\n",
            "Epoch 418/1000: Loss: 0.25\n",
            "Epoch 419/1000: Loss: 0.25\n",
            "Epoch 420/1000: Loss: 0.25\n",
            "Epoch 421/1000: Loss: 0.25\n",
            "Epoch 422/1000: Loss: 0.25\n",
            "Epoch 423/1000: Loss: 0.25\n",
            "Epoch 424/1000: Loss: 0.25\n",
            "Epoch 425/1000: Loss: 0.25\n",
            "Epoch 426/1000: Loss: 0.25\n",
            "Epoch 427/1000: Loss: 0.25\n",
            "Epoch 428/1000: Loss: 0.25\n",
            "Epoch 429/1000: Loss: 0.25\n",
            "Epoch 430/1000: Loss: 0.25\n",
            "Epoch 431/1000: Loss: 0.25\n",
            "Epoch 432/1000: Loss: 0.25\n",
            "Epoch 433/1000: Loss: 0.25\n",
            "Epoch 434/1000: Loss: 0.25\n",
            "Epoch 435/1000: Loss: 0.25\n",
            "Epoch 436/1000: Loss: 0.25\n",
            "Epoch 437/1000: Loss: 0.25\n",
            "Epoch 438/1000: Loss: 0.25\n",
            "Epoch 439/1000: Loss: 0.25\n",
            "Epoch 440/1000: Loss: 0.25\n",
            "Epoch 441/1000: Loss: 0.25\n",
            "Epoch 442/1000: Loss: 0.25\n",
            "Epoch 443/1000: Loss: 0.25\n",
            "Epoch 444/1000: Loss: 0.25\n",
            "Epoch 445/1000: Loss: 0.25\n",
            "Epoch 446/1000: Loss: 0.25\n",
            "Epoch 447/1000: Loss: 0.25\n",
            "Epoch 448/1000: Loss: 0.25\n",
            "Epoch 449/1000: Loss: 0.25\n",
            "Epoch 450/1000: Loss: 0.25\n",
            "Epoch 451/1000: Loss: 0.25\n",
            "Epoch 452/1000: Loss: 0.25\n",
            "Epoch 453/1000: Loss: 0.25\n",
            "Epoch 454/1000: Loss: 0.25\n",
            "Epoch 455/1000: Loss: 0.25\n",
            "Epoch 456/1000: Loss: 0.25\n",
            "Epoch 457/1000: Loss: 0.25\n",
            "Epoch 458/1000: Loss: 0.25\n",
            "Epoch 459/1000: Loss: 0.25\n",
            "Epoch 460/1000: Loss: 0.25\n",
            "Epoch 461/1000: Loss: 0.25\n",
            "Epoch 462/1000: Loss: 0.25\n",
            "Epoch 463/1000: Loss: 0.25\n",
            "Epoch 464/1000: Loss: 0.25\n",
            "Epoch 465/1000: Loss: 0.25\n",
            "Epoch 466/1000: Loss: 0.25\n",
            "Epoch 467/1000: Loss: 0.25\n",
            "Epoch 468/1000: Loss: 0.25\n",
            "Epoch 469/1000: Loss: 0.25\n",
            "Epoch 470/1000: Loss: 0.25\n",
            "Epoch 471/1000: Loss: 0.25\n",
            "Epoch 472/1000: Loss: 0.25\n",
            "Epoch 473/1000: Loss: 0.25\n",
            "Epoch 474/1000: Loss: 0.25\n",
            "Epoch 475/1000: Loss: 0.25\n",
            "Epoch 476/1000: Loss: 0.25\n",
            "Epoch 477/1000: Loss: 0.25\n",
            "Epoch 478/1000: Loss: 0.25\n",
            "Epoch 479/1000: Loss: 0.25\n",
            "Epoch 480/1000: Loss: 0.25\n",
            "Epoch 481/1000: Loss: 0.25\n",
            "Epoch 482/1000: Loss: 0.25\n",
            "Epoch 483/1000: Loss: 0.25\n",
            "Epoch 484/1000: Loss: 0.25\n",
            "Epoch 485/1000: Loss: 0.25\n",
            "Epoch 486/1000: Loss: 0.25\n",
            "Epoch 487/1000: Loss: 0.25\n",
            "Epoch 488/1000: Loss: 0.25\n",
            "Epoch 489/1000: Loss: 0.25\n",
            "Epoch 490/1000: Loss: 0.25\n",
            "Epoch 491/1000: Loss: 0.25\n",
            "Epoch 492/1000: Loss: 0.25\n",
            "Epoch 493/1000: Loss: 0.25\n",
            "Epoch 494/1000: Loss: 0.25\n",
            "Epoch 495/1000: Loss: 0.25\n",
            "Epoch 496/1000: Loss: 0.25\n",
            "Epoch 497/1000: Loss: 0.25\n",
            "Epoch 498/1000: Loss: 0.25\n",
            "Epoch 499/1000: Loss: 0.25\n",
            "Epoch 500/1000: Loss: 0.25\n",
            "Epoch 501/1000: Loss: 0.25\n",
            "Epoch 502/1000: Loss: 0.25\n",
            "Epoch 503/1000: Loss: 0.25\n",
            "Epoch 504/1000: Loss: 0.25\n",
            "Epoch 505/1000: Loss: 0.25\n",
            "Epoch 506/1000: Loss: 0.25\n",
            "Epoch 507/1000: Loss: 0.25\n",
            "Epoch 508/1000: Loss: 0.25\n",
            "Epoch 509/1000: Loss: 0.25\n",
            "Epoch 510/1000: Loss: 0.25\n",
            "Epoch 511/1000: Loss: 0.25\n",
            "Epoch 512/1000: Loss: 0.25\n",
            "Epoch 513/1000: Loss: 0.25\n",
            "Epoch 514/1000: Loss: 0.25\n",
            "Epoch 515/1000: Loss: 0.25\n",
            "Epoch 516/1000: Loss: 0.25\n",
            "Epoch 517/1000: Loss: 0.25\n",
            "Epoch 518/1000: Loss: 0.25\n",
            "Epoch 519/1000: Loss: 0.25\n",
            "Epoch 520/1000: Loss: 0.25\n",
            "Epoch 521/1000: Loss: 0.25\n",
            "Epoch 522/1000: Loss: 0.25\n",
            "Epoch 523/1000: Loss: 0.25\n",
            "Epoch 524/1000: Loss: 0.25\n",
            "Epoch 525/1000: Loss: 0.25\n",
            "Epoch 526/1000: Loss: 0.25\n",
            "Epoch 527/1000: Loss: 0.25\n",
            "Epoch 528/1000: Loss: 0.25\n",
            "Epoch 529/1000: Loss: 0.25\n",
            "Epoch 530/1000: Loss: 0.25\n",
            "Epoch 531/1000: Loss: 0.25\n",
            "Epoch 532/1000: Loss: 0.25\n",
            "Epoch 533/1000: Loss: 0.25\n",
            "Epoch 534/1000: Loss: 0.25\n",
            "Epoch 535/1000: Loss: 0.25\n",
            "Epoch 536/1000: Loss: 0.25\n",
            "Epoch 537/1000: Loss: 0.25\n",
            "Epoch 538/1000: Loss: 0.25\n",
            "Epoch 539/1000: Loss: 0.25\n",
            "Epoch 540/1000: Loss: 0.25\n",
            "Epoch 541/1000: Loss: 0.25\n",
            "Epoch 542/1000: Loss: 0.25\n",
            "Epoch 543/1000: Loss: 0.25\n",
            "Epoch 544/1000: Loss: 0.25\n",
            "Epoch 545/1000: Loss: 0.25\n",
            "Epoch 546/1000: Loss: 0.25\n",
            "Epoch 547/1000: Loss: 0.25\n",
            "Epoch 548/1000: Loss: 0.25\n",
            "Epoch 549/1000: Loss: 0.25\n",
            "Epoch 550/1000: Loss: 0.25\n",
            "Epoch 551/1000: Loss: 0.25\n",
            "Epoch 552/1000: Loss: 0.25\n",
            "Epoch 553/1000: Loss: 0.25\n",
            "Epoch 554/1000: Loss: 0.25\n",
            "Epoch 555/1000: Loss: 0.25\n",
            "Epoch 556/1000: Loss: 0.25\n",
            "Epoch 557/1000: Loss: 0.25\n",
            "Epoch 558/1000: Loss: 0.25\n",
            "Epoch 559/1000: Loss: 0.25\n",
            "Epoch 560/1000: Loss: 0.25\n",
            "Epoch 561/1000: Loss: 0.25\n",
            "Epoch 562/1000: Loss: 0.25\n",
            "Epoch 563/1000: Loss: 0.25\n",
            "Epoch 564/1000: Loss: 0.25\n",
            "Epoch 565/1000: Loss: 0.25\n",
            "Epoch 566/1000: Loss: 0.25\n",
            "Epoch 567/1000: Loss: 0.25\n",
            "Epoch 568/1000: Loss: 0.25\n",
            "Epoch 569/1000: Loss: 0.25\n",
            "Epoch 570/1000: Loss: 0.25\n",
            "Epoch 571/1000: Loss: 0.25\n",
            "Epoch 572/1000: Loss: 0.25\n",
            "Epoch 573/1000: Loss: 0.25\n",
            "Epoch 574/1000: Loss: 0.25\n",
            "Epoch 575/1000: Loss: 0.25\n",
            "Epoch 576/1000: Loss: 0.25\n",
            "Epoch 577/1000: Loss: 0.25\n",
            "Epoch 578/1000: Loss: 0.25\n",
            "Epoch 579/1000: Loss: 0.25\n",
            "Epoch 580/1000: Loss: 0.25\n",
            "Epoch 581/1000: Loss: 0.25\n",
            "Epoch 582/1000: Loss: 0.25\n",
            "Epoch 583/1000: Loss: 0.25\n",
            "Epoch 584/1000: Loss: 0.25\n",
            "Epoch 585/1000: Loss: 0.25\n",
            "Epoch 586/1000: Loss: 0.25\n",
            "Epoch 587/1000: Loss: 0.25\n",
            "Epoch 588/1000: Loss: 0.25\n",
            "Epoch 589/1000: Loss: 0.25\n",
            "Epoch 590/1000: Loss: 0.25\n",
            "Epoch 591/1000: Loss: 0.25\n",
            "Epoch 592/1000: Loss: 0.25\n",
            "Epoch 593/1000: Loss: 0.25\n",
            "Epoch 594/1000: Loss: 0.25\n",
            "Epoch 595/1000: Loss: 0.25\n",
            "Epoch 596/1000: Loss: 0.25\n",
            "Epoch 597/1000: Loss: 0.25\n",
            "Epoch 598/1000: Loss: 0.25\n",
            "Epoch 599/1000: Loss: 0.25\n",
            "Epoch 600/1000: Loss: 0.25\n",
            "Epoch 601/1000: Loss: 0.25\n",
            "Epoch 602/1000: Loss: 0.25\n",
            "Epoch 603/1000: Loss: 0.25\n",
            "Epoch 604/1000: Loss: 0.25\n",
            "Epoch 605/1000: Loss: 0.25\n",
            "Epoch 606/1000: Loss: 0.25\n",
            "Epoch 607/1000: Loss: 0.25\n",
            "Epoch 608/1000: Loss: 0.25\n",
            "Epoch 609/1000: Loss: 0.25\n",
            "Epoch 610/1000: Loss: 0.25\n",
            "Epoch 611/1000: Loss: 0.25\n",
            "Epoch 612/1000: Loss: 0.25\n",
            "Epoch 613/1000: Loss: 0.25\n",
            "Epoch 614/1000: Loss: 0.25\n",
            "Epoch 615/1000: Loss: 0.25\n",
            "Epoch 616/1000: Loss: 0.25\n",
            "Epoch 617/1000: Loss: 0.25\n",
            "Epoch 618/1000: Loss: 0.25\n",
            "Epoch 619/1000: Loss: 0.25\n",
            "Epoch 620/1000: Loss: 0.25\n",
            "Epoch 621/1000: Loss: 0.25\n",
            "Epoch 622/1000: Loss: 0.25\n",
            "Epoch 623/1000: Loss: 0.25\n",
            "Epoch 624/1000: Loss: 0.25\n",
            "Epoch 625/1000: Loss: 0.25\n",
            "Epoch 626/1000: Loss: 0.25\n",
            "Epoch 627/1000: Loss: 0.25\n",
            "Epoch 628/1000: Loss: 0.25\n",
            "Epoch 629/1000: Loss: 0.25\n",
            "Epoch 630/1000: Loss: 0.25\n",
            "Epoch 631/1000: Loss: 0.25\n",
            "Epoch 632/1000: Loss: 0.25\n",
            "Epoch 633/1000: Loss: 0.25\n",
            "Epoch 634/1000: Loss: 0.25\n",
            "Epoch 635/1000: Loss: 0.25\n",
            "Epoch 636/1000: Loss: 0.25\n",
            "Epoch 637/1000: Loss: 0.25\n",
            "Epoch 638/1000: Loss: 0.25\n",
            "Epoch 639/1000: Loss: 0.25\n",
            "Epoch 640/1000: Loss: 0.25\n",
            "Epoch 641/1000: Loss: 0.25\n",
            "Epoch 642/1000: Loss: 0.25\n",
            "Epoch 643/1000: Loss: 0.25\n",
            "Epoch 644/1000: Loss: 0.25\n",
            "Epoch 645/1000: Loss: 0.25\n",
            "Epoch 646/1000: Loss: 0.25\n",
            "Epoch 647/1000: Loss: 0.25\n",
            "Epoch 648/1000: Loss: 0.25\n",
            "Epoch 649/1000: Loss: 0.25\n",
            "Epoch 650/1000: Loss: 0.25\n",
            "Epoch 651/1000: Loss: 0.25\n",
            "Epoch 652/1000: Loss: 0.25\n",
            "Epoch 653/1000: Loss: 0.25\n",
            "Epoch 654/1000: Loss: 0.25\n",
            "Epoch 655/1000: Loss: 0.25\n",
            "Epoch 656/1000: Loss: 0.25\n",
            "Epoch 657/1000: Loss: 0.25\n",
            "Epoch 658/1000: Loss: 0.25\n",
            "Epoch 659/1000: Loss: 0.25\n",
            "Epoch 660/1000: Loss: 0.25\n",
            "Epoch 661/1000: Loss: 0.25\n",
            "Epoch 662/1000: Loss: 0.25\n",
            "Epoch 663/1000: Loss: 0.25\n",
            "Epoch 664/1000: Loss: 0.25\n",
            "Epoch 665/1000: Loss: 0.25\n",
            "Epoch 666/1000: Loss: 0.25\n",
            "Epoch 667/1000: Loss: 0.25\n",
            "Epoch 668/1000: Loss: 0.25\n",
            "Epoch 669/1000: Loss: 0.25\n",
            "Epoch 670/1000: Loss: 0.25\n",
            "Epoch 671/1000: Loss: 0.25\n",
            "Epoch 672/1000: Loss: 0.25\n",
            "Epoch 673/1000: Loss: 0.25\n",
            "Epoch 674/1000: Loss: 0.25\n",
            "Epoch 675/1000: Loss: 0.25\n",
            "Epoch 676/1000: Loss: 0.25\n",
            "Epoch 677/1000: Loss: 0.25\n",
            "Epoch 678/1000: Loss: 0.25\n",
            "Epoch 679/1000: Loss: 0.25\n",
            "Epoch 680/1000: Loss: 0.25\n",
            "Epoch 681/1000: Loss: 0.25\n",
            "Epoch 682/1000: Loss: 0.25\n",
            "Epoch 683/1000: Loss: 0.25\n",
            "Epoch 684/1000: Loss: 0.25\n",
            "Epoch 685/1000: Loss: 0.25\n",
            "Epoch 686/1000: Loss: 0.25\n",
            "Epoch 687/1000: Loss: 0.25\n",
            "Epoch 688/1000: Loss: 0.25\n",
            "Epoch 689/1000: Loss: 0.25\n",
            "Epoch 690/1000: Loss: 0.25\n",
            "Epoch 691/1000: Loss: 0.25\n",
            "Epoch 692/1000: Loss: 0.25\n",
            "Epoch 693/1000: Loss: 0.25\n",
            "Epoch 694/1000: Loss: 0.25\n",
            "Epoch 695/1000: Loss: 0.25\n",
            "Epoch 696/1000: Loss: 0.25\n",
            "Epoch 697/1000: Loss: 0.25\n",
            "Epoch 698/1000: Loss: 0.25\n",
            "Epoch 699/1000: Loss: 0.25\n",
            "Epoch 700/1000: Loss: 0.25\n",
            "Epoch 701/1000: Loss: 0.25\n",
            "Epoch 702/1000: Loss: 0.25\n",
            "Epoch 703/1000: Loss: 0.25\n",
            "Epoch 704/1000: Loss: 0.25\n",
            "Epoch 705/1000: Loss: 0.25\n",
            "Epoch 706/1000: Loss: 0.25\n",
            "Epoch 707/1000: Loss: 0.25\n",
            "Epoch 708/1000: Loss: 0.25\n",
            "Epoch 709/1000: Loss: 0.25\n",
            "Epoch 710/1000: Loss: 0.25\n",
            "Epoch 711/1000: Loss: 0.25\n",
            "Epoch 712/1000: Loss: 0.25\n",
            "Epoch 713/1000: Loss: 0.25\n",
            "Epoch 714/1000: Loss: 0.25\n",
            "Epoch 715/1000: Loss: 0.25\n",
            "Epoch 716/1000: Loss: 0.25\n",
            "Epoch 717/1000: Loss: 0.25\n",
            "Epoch 718/1000: Loss: 0.25\n",
            "Epoch 719/1000: Loss: 0.25\n",
            "Epoch 720/1000: Loss: 0.25\n",
            "Epoch 721/1000: Loss: 0.25\n",
            "Epoch 722/1000: Loss: 0.25\n",
            "Epoch 723/1000: Loss: 0.25\n",
            "Epoch 724/1000: Loss: 0.25\n",
            "Epoch 725/1000: Loss: 0.25\n",
            "Epoch 726/1000: Loss: 0.25\n",
            "Epoch 727/1000: Loss: 0.25\n",
            "Epoch 728/1000: Loss: 0.25\n",
            "Epoch 729/1000: Loss: 0.25\n",
            "Epoch 730/1000: Loss: 0.25\n",
            "Epoch 731/1000: Loss: 0.25\n",
            "Epoch 732/1000: Loss: 0.25\n",
            "Epoch 733/1000: Loss: 0.25\n",
            "Epoch 734/1000: Loss: 0.25\n",
            "Epoch 735/1000: Loss: 0.25\n",
            "Epoch 736/1000: Loss: 0.25\n",
            "Epoch 737/1000: Loss: 0.25\n",
            "Epoch 738/1000: Loss: 0.25\n",
            "Epoch 739/1000: Loss: 0.25\n",
            "Epoch 740/1000: Loss: 0.25\n",
            "Epoch 741/1000: Loss: 0.25\n",
            "Epoch 742/1000: Loss: 0.25\n",
            "Epoch 743/1000: Loss: 0.25\n",
            "Epoch 744/1000: Loss: 0.25\n",
            "Epoch 745/1000: Loss: 0.25\n",
            "Epoch 746/1000: Loss: 0.25\n",
            "Epoch 747/1000: Loss: 0.25\n",
            "Epoch 748/1000: Loss: 0.25\n",
            "Epoch 749/1000: Loss: 0.25\n",
            "Epoch 750/1000: Loss: 0.25\n",
            "Epoch 751/1000: Loss: 0.25\n",
            "Epoch 752/1000: Loss: 0.25\n",
            "Epoch 753/1000: Loss: 0.25\n",
            "Epoch 754/1000: Loss: 0.25\n",
            "Epoch 755/1000: Loss: 0.25\n",
            "Epoch 756/1000: Loss: 0.25\n",
            "Epoch 757/1000: Loss: 0.25\n",
            "Epoch 758/1000: Loss: 0.25\n",
            "Epoch 759/1000: Loss: 0.25\n",
            "Epoch 760/1000: Loss: 0.25\n",
            "Epoch 761/1000: Loss: 0.25\n",
            "Epoch 762/1000: Loss: 0.25\n",
            "Epoch 763/1000: Loss: 0.25\n",
            "Epoch 764/1000: Loss: 0.25\n",
            "Epoch 765/1000: Loss: 0.25\n",
            "Epoch 766/1000: Loss: 0.25\n",
            "Epoch 767/1000: Loss: 0.25\n",
            "Epoch 768/1000: Loss: 0.25\n",
            "Epoch 769/1000: Loss: 0.25\n",
            "Epoch 770/1000: Loss: 0.25\n",
            "Epoch 771/1000: Loss: 0.25\n",
            "Epoch 772/1000: Loss: 0.25\n",
            "Epoch 773/1000: Loss: 0.25\n",
            "Epoch 774/1000: Loss: 0.25\n",
            "Epoch 775/1000: Loss: 0.25\n",
            "Epoch 776/1000: Loss: 0.25\n",
            "Epoch 777/1000: Loss: 0.25\n",
            "Epoch 778/1000: Loss: 0.25\n",
            "Epoch 779/1000: Loss: 0.25\n",
            "Epoch 780/1000: Loss: 0.25\n",
            "Epoch 781/1000: Loss: 0.25\n",
            "Epoch 782/1000: Loss: 0.25\n",
            "Epoch 783/1000: Loss: 0.25\n",
            "Epoch 784/1000: Loss: 0.25\n",
            "Epoch 785/1000: Loss: 0.25\n",
            "Epoch 786/1000: Loss: 0.25\n",
            "Epoch 787/1000: Loss: 0.25\n",
            "Epoch 788/1000: Loss: 0.25\n",
            "Epoch 789/1000: Loss: 0.25\n",
            "Epoch 790/1000: Loss: 0.25\n",
            "Epoch 791/1000: Loss: 0.25\n",
            "Epoch 792/1000: Loss: 0.25\n",
            "Epoch 793/1000: Loss: 0.25\n",
            "Epoch 794/1000: Loss: 0.25\n",
            "Epoch 795/1000: Loss: 0.25\n",
            "Epoch 796/1000: Loss: 0.25\n",
            "Epoch 797/1000: Loss: 0.25\n",
            "Epoch 798/1000: Loss: 0.25\n",
            "Epoch 799/1000: Loss: 0.25\n",
            "Epoch 800/1000: Loss: 0.25\n",
            "Epoch 801/1000: Loss: 0.25\n",
            "Epoch 802/1000: Loss: 0.25\n",
            "Epoch 803/1000: Loss: 0.25\n",
            "Epoch 804/1000: Loss: 0.25\n",
            "Epoch 805/1000: Loss: 0.25\n",
            "Epoch 806/1000: Loss: 0.25\n",
            "Epoch 807/1000: Loss: 0.25\n",
            "Epoch 808/1000: Loss: 0.25\n",
            "Epoch 809/1000: Loss: 0.25\n",
            "Epoch 810/1000: Loss: 0.25\n",
            "Epoch 811/1000: Loss: 0.25\n",
            "Epoch 812/1000: Loss: 0.25\n",
            "Epoch 813/1000: Loss: 0.25\n",
            "Epoch 814/1000: Loss: 0.25\n",
            "Epoch 815/1000: Loss: 0.25\n",
            "Epoch 816/1000: Loss: 0.25\n",
            "Epoch 817/1000: Loss: 0.25\n",
            "Epoch 818/1000: Loss: 0.25\n",
            "Epoch 819/1000: Loss: 0.25\n",
            "Epoch 820/1000: Loss: 0.25\n",
            "Epoch 821/1000: Loss: 0.25\n",
            "Epoch 822/1000: Loss: 0.25\n",
            "Epoch 823/1000: Loss: 0.25\n",
            "Epoch 824/1000: Loss: 0.25\n",
            "Epoch 825/1000: Loss: 0.25\n",
            "Epoch 826/1000: Loss: 0.25\n",
            "Epoch 827/1000: Loss: 0.25\n",
            "Epoch 828/1000: Loss: 0.25\n",
            "Epoch 829/1000: Loss: 0.25\n",
            "Epoch 830/1000: Loss: 0.25\n",
            "Epoch 831/1000: Loss: 0.25\n",
            "Epoch 832/1000: Loss: 0.25\n",
            "Epoch 833/1000: Loss: 0.25\n",
            "Epoch 834/1000: Loss: 0.25\n",
            "Epoch 835/1000: Loss: 0.25\n",
            "Epoch 836/1000: Loss: 0.25\n",
            "Epoch 837/1000: Loss: 0.25\n",
            "Epoch 838/1000: Loss: 0.25\n",
            "Epoch 839/1000: Loss: 0.25\n",
            "Epoch 840/1000: Loss: 0.25\n",
            "Epoch 841/1000: Loss: 0.25\n",
            "Epoch 842/1000: Loss: 0.25\n",
            "Epoch 843/1000: Loss: 0.25\n",
            "Epoch 844/1000: Loss: 0.25\n",
            "Epoch 845/1000: Loss: 0.25\n",
            "Epoch 846/1000: Loss: 0.25\n",
            "Epoch 847/1000: Loss: 0.25\n",
            "Epoch 848/1000: Loss: 0.25\n",
            "Epoch 849/1000: Loss: 0.25\n",
            "Epoch 850/1000: Loss: 0.25\n",
            "Epoch 851/1000: Loss: 0.25\n",
            "Epoch 852/1000: Loss: 0.25\n",
            "Epoch 853/1000: Loss: 0.25\n",
            "Epoch 854/1000: Loss: 0.25\n",
            "Epoch 855/1000: Loss: 0.25\n",
            "Epoch 856/1000: Loss: 0.25\n",
            "Epoch 857/1000: Loss: 0.25\n",
            "Epoch 858/1000: Loss: 0.25\n",
            "Epoch 859/1000: Loss: 0.25\n",
            "Epoch 860/1000: Loss: 0.25\n",
            "Epoch 861/1000: Loss: 0.25\n",
            "Epoch 862/1000: Loss: 0.25\n",
            "Epoch 863/1000: Loss: 0.25\n",
            "Epoch 864/1000: Loss: 0.25\n",
            "Epoch 865/1000: Loss: 0.25\n",
            "Epoch 866/1000: Loss: 0.25\n",
            "Epoch 867/1000: Loss: 0.25\n",
            "Epoch 868/1000: Loss: 0.25\n",
            "Epoch 869/1000: Loss: 0.25\n",
            "Epoch 870/1000: Loss: 0.25\n",
            "Epoch 871/1000: Loss: 0.25\n",
            "Epoch 872/1000: Loss: 0.25\n",
            "Epoch 873/1000: Loss: 0.25\n",
            "Epoch 874/1000: Loss: 0.25\n",
            "Epoch 875/1000: Loss: 0.25\n",
            "Epoch 876/1000: Loss: 0.25\n",
            "Epoch 877/1000: Loss: 0.25\n",
            "Epoch 878/1000: Loss: 0.25\n",
            "Epoch 879/1000: Loss: 0.25\n",
            "Epoch 880/1000: Loss: 0.25\n",
            "Epoch 881/1000: Loss: 0.25\n",
            "Epoch 882/1000: Loss: 0.25\n",
            "Epoch 883/1000: Loss: 0.25\n",
            "Epoch 884/1000: Loss: 0.25\n",
            "Epoch 885/1000: Loss: 0.25\n",
            "Epoch 886/1000: Loss: 0.25\n",
            "Epoch 887/1000: Loss: 0.25\n",
            "Epoch 888/1000: Loss: 0.25\n",
            "Epoch 889/1000: Loss: 0.25\n",
            "Epoch 890/1000: Loss: 0.25\n",
            "Epoch 891/1000: Loss: 0.25\n",
            "Epoch 892/1000: Loss: 0.25\n",
            "Epoch 893/1000: Loss: 0.25\n",
            "Epoch 894/1000: Loss: 0.25\n",
            "Epoch 895/1000: Loss: 0.25\n",
            "Epoch 896/1000: Loss: 0.25\n",
            "Epoch 897/1000: Loss: 0.25\n",
            "Epoch 898/1000: Loss: 0.25\n",
            "Epoch 899/1000: Loss: 0.25\n",
            "Epoch 900/1000: Loss: 0.25\n",
            "Epoch 901/1000: Loss: 0.25\n",
            "Epoch 902/1000: Loss: 0.25\n",
            "Epoch 903/1000: Loss: 0.25\n",
            "Epoch 904/1000: Loss: 0.25\n",
            "Epoch 905/1000: Loss: 0.25\n",
            "Epoch 906/1000: Loss: 0.25\n",
            "Epoch 907/1000: Loss: 0.25\n",
            "Epoch 908/1000: Loss: 0.25\n",
            "Epoch 909/1000: Loss: 0.25\n",
            "Epoch 910/1000: Loss: 0.25\n",
            "Epoch 911/1000: Loss: 0.25\n",
            "Epoch 912/1000: Loss: 0.25\n",
            "Epoch 913/1000: Loss: 0.25\n",
            "Epoch 914/1000: Loss: 0.25\n",
            "Epoch 915/1000: Loss: 0.25\n",
            "Epoch 916/1000: Loss: 0.25\n",
            "Epoch 917/1000: Loss: 0.25\n",
            "Epoch 918/1000: Loss: 0.25\n",
            "Epoch 919/1000: Loss: 0.25\n",
            "Epoch 920/1000: Loss: 0.25\n",
            "Epoch 921/1000: Loss: 0.25\n",
            "Epoch 922/1000: Loss: 0.25\n",
            "Epoch 923/1000: Loss: 0.25\n",
            "Epoch 924/1000: Loss: 0.25\n",
            "Epoch 925/1000: Loss: 0.25\n",
            "Epoch 926/1000: Loss: 0.25\n",
            "Epoch 927/1000: Loss: 0.25\n",
            "Epoch 928/1000: Loss: 0.25\n",
            "Epoch 929/1000: Loss: 0.25\n",
            "Epoch 930/1000: Loss: 0.25\n",
            "Epoch 931/1000: Loss: 0.25\n",
            "Epoch 932/1000: Loss: 0.25\n",
            "Epoch 933/1000: Loss: 0.25\n",
            "Epoch 934/1000: Loss: 0.25\n",
            "Epoch 935/1000: Loss: 0.25\n",
            "Epoch 936/1000: Loss: 0.25\n",
            "Epoch 937/1000: Loss: 0.25\n",
            "Epoch 938/1000: Loss: 0.25\n",
            "Epoch 939/1000: Loss: 0.25\n",
            "Epoch 940/1000: Loss: 0.25\n",
            "Epoch 941/1000: Loss: 0.25\n",
            "Epoch 942/1000: Loss: 0.25\n",
            "Epoch 943/1000: Loss: 0.25\n",
            "Epoch 944/1000: Loss: 0.25\n",
            "Epoch 945/1000: Loss: 0.25\n",
            "Epoch 946/1000: Loss: 0.25\n",
            "Epoch 947/1000: Loss: 0.25\n",
            "Epoch 948/1000: Loss: 0.25\n",
            "Epoch 949/1000: Loss: 0.25\n",
            "Epoch 950/1000: Loss: 0.25\n",
            "Epoch 951/1000: Loss: 0.25\n",
            "Epoch 952/1000: Loss: 0.25\n",
            "Epoch 953/1000: Loss: 0.25\n",
            "Epoch 954/1000: Loss: 0.25\n",
            "Epoch 955/1000: Loss: 0.25\n",
            "Epoch 956/1000: Loss: 0.25\n",
            "Epoch 957/1000: Loss: 0.25\n",
            "Epoch 958/1000: Loss: 0.25\n",
            "Epoch 959/1000: Loss: 0.25\n",
            "Epoch 960/1000: Loss: 0.25\n",
            "Epoch 961/1000: Loss: 0.25\n",
            "Epoch 962/1000: Loss: 0.25\n",
            "Epoch 963/1000: Loss: 0.25\n",
            "Epoch 964/1000: Loss: 0.25\n",
            "Epoch 965/1000: Loss: 0.25\n",
            "Epoch 966/1000: Loss: 0.25\n",
            "Epoch 967/1000: Loss: 0.25\n",
            "Epoch 968/1000: Loss: 0.25\n",
            "Epoch 969/1000: Loss: 0.25\n",
            "Epoch 970/1000: Loss: 0.25\n",
            "Epoch 971/1000: Loss: 0.25\n",
            "Epoch 972/1000: Loss: 0.25\n",
            "Epoch 973/1000: Loss: 0.25\n",
            "Epoch 974/1000: Loss: 0.25\n",
            "Epoch 975/1000: Loss: 0.25\n",
            "Epoch 976/1000: Loss: 0.25\n",
            "Epoch 977/1000: Loss: 0.25\n",
            "Epoch 978/1000: Loss: 0.25\n",
            "Epoch 979/1000: Loss: 0.25\n",
            "Epoch 980/1000: Loss: 0.25\n",
            "Epoch 981/1000: Loss: 0.25\n",
            "Epoch 982/1000: Loss: 0.25\n",
            "Epoch 983/1000: Loss: 0.25\n",
            "Epoch 984/1000: Loss: 0.25\n",
            "Epoch 985/1000: Loss: 0.25\n",
            "Epoch 986/1000: Loss: 0.25\n",
            "Epoch 987/1000: Loss: 0.25\n",
            "Epoch 988/1000: Loss: 0.25\n",
            "Epoch 989/1000: Loss: 0.25\n",
            "Epoch 990/1000: Loss: 0.25\n",
            "Epoch 991/1000: Loss: 0.25\n",
            "Epoch 992/1000: Loss: 0.25\n",
            "Epoch 993/1000: Loss: 0.25\n",
            "Epoch 994/1000: Loss: 0.25\n",
            "Epoch 995/1000: Loss: 0.25\n",
            "Epoch 996/1000: Loss: 0.25\n",
            "Epoch 997/1000: Loss: 0.25\n",
            "Epoch 998/1000: Loss: 0.25\n",
            "Epoch 999/1000: Loss: 0.25\n"
          ]
        }
      ],
      "source": [
        "batch_size = 4\n",
        "train_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#Let us try running for 1000 epochs using gradient descent and check if we can reach 0  loss\n",
        "epochs = 1000\n",
        "for i in range(epochs):\n",
        "    # Iterate through training dataloader\n",
        "    for x,y in train_loader:\n",
        "        # Generate Prediction\n",
        "        preds = model(x)\n",
        "        # Get the loss and perform backpropagation\n",
        "        loss = mse_loss(y,preds)\n",
        "        loss.backward()\n",
        "        # Let's update the weights\n",
        "        with torch.no_grad():\n",
        "            w -= w.grad *0.1\n",
        "            b -= b.grad * 0.1\n",
        "            # Set the gradients to zero\n",
        "            w.grad.zero_()\n",
        "            b.grad.zero_()\n",
        "    print(f\"Epoch {i}/{epochs}: Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7c51535a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c51535a",
        "outputId": "24804093-3f8e-4cf9-874e-598733391077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction is :n tensor([0.5000, 0.5000, 0.5000, 0.5000], grad_fn=<AddBackward0>)\n",
            "nActual targets is :n tensor([1., 0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "for x,y in train_loader:\n",
        "    preds = model(x)\n",
        "    print(\"Prediction is :n\",preds)\n",
        "    print(\"nActual targets is :n\",y)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e1f6ed13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f6ed13",
        "outputId": "e97296ae-96cf-421d-8adb-6b7dccd40577"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.2834e-07, 1.3101e-07], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Finding optimal weight values\n",
        "w "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6b140813",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b140813",
        "outputId": "afef3c0f-b072-4eb1-bff3-ed6a8b10f939"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5000, requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#Finding optimal value of bias\n",
        "b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5764c0d2",
      "metadata": {
        "id": "5764c0d2"
      },
      "source": [
        "## 2.1\n",
        "\n",
        "2.1 Given a 1-d input x=[1,-1,3,4,4] and a kernel=[1,1], find the 1-d convolution for stride=1 and stride=2. Solve both on paper and using pyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f5484346",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5484346",
        "outputId": "1b3a6b25-0d3d-4021-bf94-e1904bbbf8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So, for stride=1, the convolved output is: tensor([[0., 2., 7., 8.]], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "x1 = torch.tensor([1,-1,3,4,4], dtype=torch.float)\n",
        "x1=x1.unsqueeze(0)\n",
        "x1=x1.unsqueeze(0)\n",
        "x1=x1.squeeze(0)\n",
        "cnn1d = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2,bias=False)\n",
        "cnn1d.weight.data=torch.tensor([1,1],dtype=torch.float,requires_grad=True)\n",
        "cnn1d.weight.data=cnn1d.weight.data.unsqueeze(0).unsqueeze(0)\n",
        "cnn1d.weight.data\n",
        "out_1=cnn1d(x1)\n",
        "print(\"So, for stride=1, the convolved output is:\",out_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "42f77af1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42f77af1",
        "outputId": "c69066db-78eb-49ee-b703-fdc8c56c5379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "So, for stride =2, the convolved output is: tensor([[0., 7.]], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "cnn1d = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2, bias=False)\n",
        "cnn1d.weight.data=torch.tensor([1,1],dtype=torch.float,requires_grad=True)\n",
        "cnn1d.weight.data=cnn1d.weight.data.unsqueeze(0).unsqueeze(0)\n",
        "out_2=cnn1d(x1)\n",
        "print(\"So, for stride =2, the convolved output is:\",out_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dec04d20",
      "metadata": {
        "id": "dec04d20"
      },
      "source": [
        "## 2.2\n",
        "\n",
        "You’re given the following grayscale image (any matrix in 2 dimensions is a grayscale image):\n",
        "\n",
        "i) You’re given 2 2*2 Filters (kernels): [[1,0],[0,1]] and [[0,1],[1,0]]. Find the output when the image is convolved using each Filter with stride=1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a70a5218",
      "metadata": {
        "id": "a70a5218"
      },
      "outputs": [],
      "source": [
        "x3 = torch.tensor([[0.1,-0.6,0.4,0.8],[-0.4,0.3,0.9,0.2],[0.5,0.2,0.8,-0.7],[0.3,0.7,-0.4,0.1]])\n",
        "#Flattening input\n",
        "x3=x3.unsqueeze(0)\n",
        "cnn2d_2=nn.Conv2d(in_channels=1,out_channels=2,kernel_size=2,bias=False)\n",
        "cnn2d_2.weight.data=torch.tensor([[[1,0],[0,1]],\n",
        "                                  \n",
        "                                  [[0,1],[1,0]]   ],dtype=torch.float)\n",
        "cnn2d_2.weight.data=cnn2d_2.weight.data.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e4bc1600",
      "metadata": {
        "id": "e4bc1600"
      },
      "outputs": [],
      "source": [
        "out=cnn2d_2(x3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d64d0692",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d64d0692",
        "outputId": "b4adfdb4-873d-4d16-d18f-ff1b61b25281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Convolved Output Image for both channels is:\n",
            " tensor([[[ 0.4000,  0.3000,  0.6000],\n",
            "         [-0.2000,  1.1000,  0.2000],\n",
            "         [ 1.2000, -0.2000,  0.9000]],\n",
            "\n",
            "        [[-1.0000,  0.7000,  1.7000],\n",
            "         [ 0.8000,  1.1000,  1.0000],\n",
            "         [ 0.5000,  1.5000, -1.1000]]], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "print(\"Convolved Output Image for both channels is:\\n\",out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6573c07",
      "metadata": {
        "id": "e6573c07"
      },
      "source": [
        "ii) What are the dimensions of each channel in (i)? What will be the dimensions of the output when stride=2?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "502eb1f8",
      "metadata": {
        "id": "502eb1f8",
        "outputId": "603f8d78-1792-4b4b-89ae-85b7f1293ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output for stride=2 is tensor([[[ 0.4000,  0.6000],\n",
            "         [ 1.2000,  0.9000]],\n",
            "\n",
            "        [[-1.0000,  1.7000],\n",
            "         [ 0.5000, -1.1000]]], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "cnn2d_2_std2=nn.Conv2d(in_channels=1,out_channels=1,kernel_size=2,stride=2,bias=False)\n",
        "cnn2d_2_std2.weight.data=torch.tensor([[[1,0],[0,1]],\n",
        "                                  \n",
        "                                  [[0,1],[1,0]]   ],dtype=torch.float)\n",
        "cnn2d_2_std2.weight.data=cnn2d_2_std2.weight.data.unsqueeze(1)\n",
        "out=cnn2d_2_std2(x3)\n",
        "print(\"Output for stride=2 is\",out) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, dimension of each channel when stride=1 is 3x3 and when stride=2 is 2x2"
      ],
      "metadata": {
        "id": "MIesIUVWJ-c0"
      },
      "id": "MIesIUVWJ-c0"
    },
    {
      "cell_type": "markdown",
      "id": "feb0869d",
      "metadata": {
        "id": "feb0869d"
      },
      "source": [
        "### iii) The given image is a 2-d matrix. How can you convolve it so that the output channel has only one dimension?\n",
        "\n",
        "Solution is given in attached PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6756a244",
      "metadata": {
        "id": "6756a244"
      },
      "source": [
        "### iv) Perform a 2*2 max-pooling with stride=1 on the image both on paper and using PyTorch. What are the dimensions of the channel after max-pooling?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "066e4682",
      "metadata": {
        "id": "066e4682"
      },
      "outputs": [],
      "source": [
        "maxpool = nn.MaxPool2d(2, stride=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f6bb4c9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6bb4c9c",
        "outputId": "eeb95026-7024-4724-cfe6-2291147f9738"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.3000, 0.9000, 0.9000],\n",
              "         [0.5000, 0.9000, 0.9000],\n",
              "         [0.7000, 0.8000, 0.8000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "out=maxpool(x3)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0ed1ab35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ed1ab35",
        "outputId": "736afab0-eac2-4dfa-b26a-ac60f4372475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of channel after max pooling torch.Size([1, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "print(\"Dimensions of channel after max pooling\",out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra-credit\n",
        "If the size of the output channel after performing a 2*2 convolution (stride=1) and a 2*2 max-pooling (stride=1) is the same, why do you think we needed max-pooling when we could perform convolution to decrease the size\n",
        "of the image. Think in terms of the advantages that max-pooling may o\u0000er over convolution. When do you think a max-pooling operation may not be advantageous (and in fact may hurt the network)?\n",
        "\n",
        "Solution is given in the attached PDF File"
      ],
      "metadata": {
        "id": "UsxwQdI53PXk"
      },
      "id": "UsxwQdI53PXk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 :\n",
        "\n",
        "All answers have been solved in the attached PDF File."
      ],
      "metadata": {
        "id": "LzpZr8SG3fWY"
      },
      "id": "LzpZr8SG3fWY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}